<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vehicle Detection and Classification Proposal</title>
    <link rel="stylesheet" href="index.css">
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Vehicle Detection and Classification Proposal</h1>
            <h3>Jiya Varma, Madeline Liu Hou, Mengzhu Ou, Nicole Hernandez Canales, Sophia Isabel Marples Rodriguez</h3>
            <a href="https://github.com/mengzhuou/CS4641_CarClassification" class="btn" target="_blank">View on GitHub</a>
        </div>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>Vehicle detection and classification are crucial for traffic control, management, and surveillance, but manual monitoring of traffic footage is inefficient, considering the volume of vehicles in high-traffic areas. Automated vehicle classification systems can address this problem by using machine learning algorithms to detect and categorize vehicles in real-time.</p><br/>
            <h3>Literature Review</h3>
            <p>
                Automated vehicle classification systems remain difficult to implement, considering issues such as the computational constraints of video processing, the effect of poor weather and environmental conditions, and occlusion [1]. Researchers have focused on improving vehicle detection using machine learning classifiers and deep learning methods, such as CNN-based methods, YOLO, and Vision Transformer (ViT) techniques [1], [2], [3].
            </p><br/>

            <h3>Dataset Description</h3>
            <p>We plan to use the Stanford Cars dataset for training our models. The dataset contains 16,185 images across 196 different classes of cars, with annotations for each class describing the make, model, and year of the car. </p><br/>
            <li><a href="https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset" target="_blank">Dataset Link</a></li>
            </ul>
        </section>

        <section id="problem-definition">
            <h2>Problem Definition</h2>
            <p>In our project, we seek to identify and classify cars from an image, and also to detect their license plate. As previously covered, the uses of license plate detection and car classification range from law enforcement to traffic control. Our project seeks to improve upon existing methods.</p>
        </section>
        
        <section id="gantt">
            <h2>Gantt Chart</h2>
            <a href="https://gtvault-my.sharepoint.com/:x:/r/personal/srodriguez67_gatech_edu/_layouts/15/Doc.aspx?sourcedoc=%7B83F8A18E-E3EF-4F5F-86D2-9EF9D988573C%7D&file=GanttChart.xlsx&fromShare=true&action=default&mobileredirect=true" target="_blank">Gantt Chart Link</a>
        </section>

        <section id="methods">
            <h2>Implemented Methods</h2>
        
            <p>For our first model implementation, we decided to create a Residual Network Model, or ResNet, based on Microsoft Research's 2015 publication. The core concept of the ResNet model is the residual block, which creates a direct connection from some input to a deeper convolution layer's output, essentially "skipping" computations in between. This is useful in preventing vanishing or exploding gradients that often occur in deeper architectures due to high frequency of multiplications. The skip connections ensure that the gradients are kept secure from becoming too small or large, and allow for an easier flow path during backpropagation so that the network can more effectively adjust the weights. This is effective for producing a generalizable model as it can learn patterns across deeper layers without getting lost [8].</p>
            <br>
            <p>Our residual block contains 2x convolution (with rectified linear unit, or ReLu activation) + batch norm blocks such that the input tensor is added to the result of the second convolution output, and then finally passes through a ReLu activation before being returned.</p>
            <br>
            <p>The current architecture of our ResNet Model is as follows:
            [input tensor -> data augmentation layers -> initial convolution -> batch norm -> activation -> residual block 1 -> residual block 2 -> residual block 3 -> global average pooling -> flatten -> dense layer (with softmax)] [10,11,12]</p>
            <br>
            <p>We increase the filter size between each residual block to capture more complex features of our car image dataset, (theoretically) such as chassis shape or car brand logos [9]. To offset the computational cost of increased number of feature maps and maintain uniform dimensions, we employ a projection or bottleneck convolution layer with a 1x1 kernel size that ensures that the number of channels/depth of the input matches the # of channels of the residual block's second convolution's output [15].</p>
            <br>
            <p>We also employ Sparse Categorical cross entropy as our loss function as it allows direct indexing of class labels rather than requiring one-hot encoding, which is helpful for our pipeline as we have a large class size of 48 [14]. Additionally, we opted to use 'adam' (adaptive moment estimator) as our optimizer as it allows more flexibility in adjusting learning rates based on how large/small the gradients are. We have found that 'adam' is a good first choice due to its faster convergence speed and applicability to several problems without extensive need for hyperparameter tuning [13]. However, we plan to run several hyperparameter tuning experiments with various optimizers, learning rates, momentum decay, etc in order to improve our model performance.</p>
        </section>

        <section id="methods">
            <h2>Data Pre-processing/Loading</h2>
    
            <p>The original Stanford Cars dataset evenly split images between test and train sets but did not include a validation set. To address this, we decided to adjust the distribution to a 80-10-10 split—80% for training, 10% for validation, and 10% for testing, also shifting the majority of images to our training set. We combined each car’s test and train images into a unified set. From this merged set, we randomly divided the images into the new 80-10-10 distribution.</p>
            <br>
            <p>To improve our model’s accuracy, we implemented the following data preprocessing steps:</p><br>
            <ul>
                <li><strong>Batching:</strong> Our train, test, and validation datasets were divided in smaller batches of 32 images each, reducing memory usage and allowing faster training and faster convergence of our model [16].</li>
                <li><strong>Normalization of pixels:</strong> The pixel values of each image were normalized by dividing each pixel value by 255, resulting in values within the range [0, 1]. This helped maintain stable gradients and improved computation speed [17].</li>
                <li><strong>Data prefetching:</strong> Prefetching allowed the model to dynamically load the next batches based on current resources, minimizing wait time between batches. This allowed for faster training and optimized resource usage [18].</li>
                <li><strong>Data augmentation:</strong> The training dataset was synthetically increased with data augmentation techniques, including random horizontal flips, and slight rotations and slight zooms to reduce overfitting [19].</li>
            </ul>
        </section>


        <section id="results-discussion">
            <h2>Results and Discussion</h2>
        
            <p>The ResNet model implemented here is inspired by Microsoft's 2015 ResNet architecture, leveraging residual blocks to enable deep learning without gradient vanishing or explosion issues. Each residual block in the model comprises two convolutional layers with batch normalization, connected by skip connections. These skip connections help preserve gradients across layers, allowing for more stable and effective training even in deep networks. The model processes input data through an initial convolution, followed by three residual blocks that increase the number of filters to capture complex features. A bottleneck 1x1 convolution aligns the dimensions between residual blocks to keep the feature map size consistent, which is essential for efficient learning without inflating computational costs. The model concludes with a global average pooling layer, flattening, and a dense softmax layer for classification. The use of sparse categorical cross-entropy as the loss function helps simplify label handling, and the Adam optimizer accelerates convergence with minimal hyperparameter adjustments.</p>
            <br>

            <h3>Initial Evaluation (0% Accuracy)</h3>
            <p>The first run, with 196 classes and no data augmentation, yielded 0% test accuracy (initally 1% at epoch 1, but decreased with each epoch) and no meaningful precision, recall, or F1 scores across classes. This was largely due to the high class count, which overwhelmed the model, and the computational burden from the 1x1 bottleneck convolution layer, which introduced inefficiencies. The confusion matrix revealed nearly random predictions, indicating severe underfitting. At this stage, the model lacked the ability to generalize effectively, likely due to insufficient gradient flow and difficulty learning distinctive features for each class.</p>
            <br>

            <h3>Second Evaluation (13% Accuracy, Reduced Classes)</h3>
            <p>Reducing the class count from 196 to 48 improved the model's ability to distinguish between brands, resulting in a 13% accuracy increase. This reduction made the task more manageable, allowing the model to start learning specific brand features, though still at a limited level. Precision and recall scores showed modest improvement, particularly for brands like Aston Martin, Ferrari, and Chevrolet. The confusion matrix demonstrated an increase in correctly predicted classes but still displayed significant misclassifications, suggesting further optimization was needed.</p>
            <br>

            <h3>Third Evaluation (17% Accuracy, Added Data Augmentation)</h3>
            <p>Adding data augmentation layers led to a 17% test accuracy, as augmentation introduced variability in the training set, allowing the model to generalize better across different lighting, contrast, and orientation conditions. Brands like Audi, BMW, Chrysler, and Ferrari saw further improvement in precision, recall, and F1 scores, indicating that the model was increasingly capable of recognizing brand-specific features. The confusion matrix showed increased clustering along the diagonal, especially for brands with a substantial number of samples, though performance on less-represented brands remained low.</p>
            <br>

            <h3>Fourth Evaluation (32% Accuracy, 30 Epochs)</h3>
            <p>The model reached 32% accuracy after training for 30 epochs, allowing it to better internalize features across brands. This run saw notable improvements in both precision and recall for brands like Acura, Audi, BMW, Ferrari, and HUMMER. The longer training time allowed the model to recognize distinctive patterns, but brands with limited data still had poor performance, as reflected in the confusion matrix. This indicates a need for techniques to address class imbalance, such as weighted loss functions, to further improve generalization.</p>
            <br>

            <h3>Fifth Evaluation (38% Accuracy, 50 Epochs, Overfitting Observed)</h3>
            <p>With 50 epochs, the model achieved a 38% test accuracy and an F1 score of 0.37. However, training accuracy reached 70%, suggesting overfitting as the validation accuracy stagnated at 36%. This was confirmed in the confusion matrix, where frequently seen brands showed strong performance while rarer brands remained problematic. The model learned specific details rather than general patterns, which compromised its ability to generalize. Regularization methods, such as dropout or weight decay, may help prevent this overfitting in future runs.</p>
            <br>

            <h3>Sixth Evaluation (24% Accuracy, Extra Activation Layer)</h3>
            <p>Adding a separate activation layer on top of batch normalization within the residual blocks reduced test accuracy to 24% and F1 score to 0.2, indicating degraded model performance. The addition of an extra activation layer disrupted the learning dynamics within residual blocks, as seen in the lower scores across several brands. The confusion matrix shows fewer correct predictions than in the 32% model, particularly for brands previously performing well, like BMW and Ferrari. This outcome suggests that integrating activation functions directly within convolutional layers may provide more stable learning.</p>
            <br><br>

            <h3>Summary Table of Model Runs and Results</h3>

            <table>
                <thead>
                    <tr>
                        <th>Evaluation</th>
                        <th>Model Changes</th>
                        <th>Test Accuracy</th>
                        <th>Precision</th>
                        <th>Recall</th>
                        <th>F1 Score</th>
                        <th>Observations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Initial (model_916520)<br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_916520/classification_report_20241107_120051.txt" target="_blank">Classification Report</a>
                            <br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_916520/confusion_matrix_20241107_120051.png" target="_blank">Confusion Matrix Heat Map</a>
                        </td>
                        <td>196 classes, no data augmentation</td>
                        <td>1%</td>
                        <td>0.00</td>
                        <td>0.00</td>
                        <td>0.00</td>
                        <td>Severe underfitting with random-like predictions; high computational burden from 1x1 convolution bottleneck</td>
                    </tr>
                    <tr>
                        <td>Second (model_916991)<br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_condensedclasssize_916991/classification_report_20241107_164005.txt" target="_blank">Classification Report</a>
                            <br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_condensedclasssize_916991/confusion_matrix_20241107_164005.png" target="_blank">Confusion Matrix Heat Map</a>
                        </td>
                        <td>Reduced to 48 classes, 10 epochs, 10 steps/epoch</td>
                        <td>13%</td>
                        <td>0.11</td>
                        <td>0.13</td>
                        <td>0.07</td>
                        <td>Improved ability to distinguish brands; modest improvement for certain brands like Aston Martin and Ferrari; some correct predictions appeared</td>
                    </tr>
                    <tr>
                        <td>Third (model_917357)<br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_917357/classification_report_20241107_194802.txt" target="_blank">Classification Report</a>
                            <br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_917357/confusion_matrix_20241107_194802.png" target="_blank">Confusion Matrix Heat Map</a>
                        </td>
                        <td>Added data augmentation</td>
                        <td>17%</td>
                        <td>0.14</td>
                        <td>0.17</td>
                        <td>0.13</td>
                        <td>Better generalization across varied lighting and contrast; increased performance for Audi, BMW, and Chrysler; improved diagonal clustering</td>
                    </tr>
                    <tr>
                        <td>Fourth (model_917372)<br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_32accuracy_917372/classification_report_20241107_205928.txt" target="_blank">Classification Report</a>
                            <br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_32accuracy_917372/confusion_matrix_20241107_205928.png" target="_blank">Confusion Matrix Heat Map</a>
                        </td>
                        <td>30 epochs full steps</td>
                        <td>32%</td>
                        <td>0.36</td>
                        <td>0.32</td>
                        <td>0.29</td>
                        <td>Significant performance gain; better feature extraction for major brands; less-represented brands still underperformed</td>
                    </tr>
                    <tr>
                        <td>Fifth (model_917799)<br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_917799/classification_report_20241108_010740.txt" target="_blank">Classification Report</a>
                            <br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_917799/confusion_matrix_20241108_010740.png" target="_blank">Confusion Matrix Heat Map</a>
                        </td>
                        <td>50 epochs full steps, observed overfitting</td>
                        <td>38%</td>
                        <td>0.47</td>
                        <td>0.38</td>
                        <td>0.37</td>
                        <td>Highest accuracy but clear overfitting with 70% train accuracy; frequent brands well-identified, less common brands poorly predicted</td>
                    </tr>
                    <tr>
                        <td>Sixth (model_926034)<br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_926034/classification_report_20241111_144402.txt" target="_blank">Classification Report</a>
                            <br><br>
                            <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/results/model_926034/confusion_matrix_20241111_144402.png" target="_blank">Confusion Matrix Heat Map</a>
                        </td>
                        <td>Extra activation layer after batch norm</td>
                        <td>24%</td>
                        <td>0.24</td>
                        <td>0.24</td>
                        <td>0.20</td>
                        <td>Performance declined with extra activation; reduced accuracy for most brands; activation within convolution layers preferable for stability</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="reference">
            <h2>References</h2>
            <table class="reference-table">
                <tbody>
                    <tr>
                        <td>[1] S.S. Sarikan, A.M. Obayoglu, and O. Zilci, “Automated Vehicle Classification with Image Processing and Computational Intelligence,” Procedia Computer Science, vol. 114, pp. 515-522, 2017, doi: https://doi.org/10.1016/j.procs.2017.09.022.  </td>
                    </tr>
                    <tr>
                        <td>[2] W. Maungmai and C. Nuthong, "Vehicle Classification with Deep Learning,"2019 IEEE 4th International Conference on Computer and Communication Systems (ICCCS), Singapore, 2019, pp. 294-298, doi: 10.1109/CCOMS.2019.8821689.
                        </td>
                    </tr>
                    <tr>
                        <td>[3] Liang L, Ma H, Zhao L, Xie X, Hua C, Zhang M,and  Zhang Y, “Vehicle Detection Algorithms for Autonomous Driving: A Review”, Sensors (Basel). Vol 10, May, pp. 13-24, 2024, doi: 10.3390/s24103088.
                        </td>
                    </tr>
                    <tr>
                        <td>[4] M. Patel, “The Complete Guide to Image Preprocessing Techniques in Python,” Medium, Oct. 23, 2023. https://medium.com/@maahip1304/the-complete-guide-to-image-preprocessing-techniques-in-python-dca30804550c
                        </td>
                    </tr>
                    <tr>
                        <td> [5] “11.8. Transformers for Vision — Dive into Deep Learning 1.0.3 documentation,” d2l.ai. https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html
                        </td>
                    </tr>
                    <tr>
                        <td> [6] A. A. M. Omer, “Image Classification Based on Vision Transformer,” Journal of Computer and Communications, vol. 12, no. 4, pp. 49-59, Apr. 2024, doi: https://doi.org/10.4236/jcc.2024.124005.
                        </td>
                    </tr>
                    <tr>
                        <td>[8] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” arXiv.org, Dec. 10, 2015. https://arxiv.org/abs/1512.03385</td>
                    </tr>
                    <tr>
                        <td>[9] “(PDF) The Impact of Filter Size and Number of Filters on Classification Accuracy in CNN,” www.researchgate.net. https://www.researchgate.net/publication/342999107_The_Impact_of_Filter_Size_and_Number_of_Filters_on_Classification_Accuracy_in_CNN</td>
                    </tr>
                    <tr>
                        <td>[10] https://medium.com/swlh/how-to-create-a-residual-network-in-tensorflow-and-keras-cd97f6c62557</td>
                    </tr>
                    <tr>
                        <td>[11] https://github.com/Prakadeeswaran05/Customising-your-models-with-TensorFlow-2-Coursera/blob/main/Week%204%20Programming%20Assignment-Residual%20Network.ipynb</td>
                    </tr>
                    <tr>
                        <td>[12] https://www.youtube.com/watch?v=Q1JCrG1bJ-A</td>
                    </tr>
                    <tr>
                        <td>[13] https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam</td>
                    </tr>
                    <tr>
                        <td>[14] https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy</td>
                    </tr>
                    <tr>
                        <td>[15] https://www.geeksforgeeks.org/what-is-a-projection-layer-in-the-context-of-neural-networks/</td>
                    </tr>
                    <tr>
                        <td>[16] “Batch processing: The key to making your neural networks sing,” Tooliqa, https://www.tooli.qa/insights/batch-processing-the-key-to-making-your-neural-networks-sing (accessed Nov. 11, 2024).</td>
                    </tr>
                    <tr>
                        <td>[17] P. Harsh, “Normalization in image preprocessing: Scaling pixel values by 1/255,” Medium, https://medium.com/@patelharsh7458/normalization-in-image-preprocessing-scaling-pixel-values-by-1-255-111b2fa496d4 (accessed Nov. 11, 2024).</td>
                    </tr>
                    <tr>
                        <td>[18] “Better performance with the tf.data API,” TensorFlow, https://www.tensorflow.org/guide/data_performance (accessed Nov. 11, 2024).</td>
                    </tr>
                    <tr>
                        <td>[19] “Data augmentation,” TensorFlow, https://www.tensorflow.org/tutorials/images/data_augmentation (accessed Nov. 11, 2024).</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="contribution">
            <h2>Midterm Contributions</h2>
            <table>
                <thead>
                    <tr>
                        <th>Name</th>
                        <th>Midterm Contributions</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Madeline Liu Hou</td>
                        <td>Introduction and data-preprocessing/loading section, data split/merge and data augmentation </td>
                    </tr>
                    <tr>
                        <td>Sophia Isabel Marples Rodriguez</td>
                        <td>Problem Definition section</td>
                    </tr>
                    <tr>
                        <td>Jiya Varma</td>
                        <td>Methods section, PowerPoint Slides, ResNet model implementation and training </td>
                    </tr>
                    <tr>
                        <td>Nicole Hernandez Canales</td>
                        <td>Results and Discussion section and <a href="https://github.com/mengzhuou/CS4641_CarClassification/blob/main/resnet_exp.py" target="_blank">evaluate model and display results portion</a></td>
                    </tr>
                    <tr>
                        <td>Mengzhu Ou</td>
                        <td>Data Cleaning</td>
                    </tr>
                </tbody>
            </table>
        </section>
    </main>
</body>
</html>
